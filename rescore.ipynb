{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b512d7f6",
   "metadata": {},
   "source": [
    "# Rescore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9fd020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from time import sleep, time\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696b7d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "origindf = pd.read_excel('score.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9038c411",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataExtraction:\n",
    "  def __init__(self, df:pd.DataFrame) -> None:\n",
    "    self._model = os.getenv(\"LLM_MODEL\")\n",
    "    self._seed = os.getenv(\"SEED\")\n",
    "    self._df = df.copy()\n",
    "    self._df['llm'] = self._df[self._model]\n",
    "  \n",
    "  def get_df(self):\n",
    "    return self._df\n",
    "\n",
    "  def get_row_by_index(self, i:int):\n",
    "    return self._df.iloc[i]\n",
    "    \n",
    "  def get_url_by_index(self, i:int):\n",
    "    row = self._df.iloc[i]\n",
    "    return {\n",
    "      'citing':f'https://www.semanticscholar.org/paper/{row.paper_id}',\n",
    "      'cited':f'https://www.semanticscholar.org/paper/{row.cited_id}'\n",
    "    }\n",
    "\n",
    "  def high_cross_high_llm(self):\n",
    "    df = self._df\n",
    "    cand = df[(df['relevance_cross']>=0.95) & (df[\"llm\"] >= 0.95)]\n",
    "    return cand[['relevance_cross','llm']]\n",
    "  def high_cross_mid_llm(self):\n",
    "    df = self._df\n",
    "    cand = df[(df['relevance_cross']>=0.8) & (df[\"llm\"] >= 0.40) & (df[\"llm\"] <= 0.6)]\n",
    "    return cand[['relevance_cross','llm']]\n",
    "  def high_cross_low_llm(self):\n",
    "    df = self._df\n",
    "    cand = df[(df['relevance_cross']>=0.8) & (df[\"llm\"] <= 0.2)]\n",
    "    return cand[['relevance_cross','llm']]\n",
    "\n",
    "  def low_cross_low_llm(self):\n",
    "    df = self._df\n",
    "    cand = df[(df['relevance_cross']<=0.1) & (df[\"llm\"] <= 0.1)]\n",
    "    return cand[['relevance_cross','llm']]\n",
    "  def low_cross_mid_llm(self):\n",
    "    df = self._df\n",
    "    cand = df[(df['relevance_cross']<=0.2) & (df[\"llm\"] >= 0.40) & (df[\"llm\"] <= 0.6)]\n",
    "    return cand[['relevance_cross','llm']]\n",
    "  def low_cross_high_llm(self):\n",
    "    df = self._df\n",
    "    cand = df[(df['relevance_cross']<=0.2) & (df[\"llm\"] >= 0.9)]\n",
    "    return cand[['relevance_cross','llm']]\n",
    "\n",
    "  def select_case(self, cross_cond, llm_target, tol=0.05):\n",
    "    \"\"\"\n",
    "    从 df 中选择一个样本：\n",
    "    - cross_cond: lambda x: bool\n",
    "    - llm_target: 目标 llm 值（0.8 / 0.5 / 0.2）\n",
    "    - tol: 允许误差\n",
    "    \"\"\"\n",
    "    df = self._df\n",
    "    cand = df[df[\"relevance_cross\"].apply(cross_cond)]\n",
    "    cand = cand[(cand[\"llm\"] >= llm_target - tol) & (cand[\"llm\"] <= llm_target + tol)]\n",
    "    return cand.sample(1).iloc[0] if len(cand) > 0 else None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce830ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_instance = DataExtraction(origindf)\n",
    "data_instance.high_cross_high_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900a1013",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_instance.get_url_by_index(109)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a887b312",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMRescorer:\n",
    "    \"\"\"\n",
    "    LLM-based rescore module for DAO arbitration.\n",
    "\n",
    "    L_ij^re = F_LLM(P_i_full, P_j_full)\n",
    "\n",
    "    This scorer is used only in rescore / arbitration,\n",
    "    not in routine citation scoring.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.client = OpenAI(\n",
    "            api_key=os.getenv(\"LLM_KEY\"),\n",
    "            base_url=os.getenv(\"LLM_URL\")\n",
    "        )\n",
    "        self.model = os.getenv(\"LLM_MODEL\")\n",
    "\n",
    "    def rescore(self, contexts:str, citing_text: str, cited_text: str) -> dict:\n",
    "        \"\"\"\n",
    "        Perform full-paper semantic rescore based on two papers' full texts.\n",
    "\n",
    "        Inputs:\n",
    "        - citing_text: extracted text of the citing paper (PDF -> text)\n",
    "        - cited_text: extracted text of the cited paper (PDF -> text)\n",
    "\n",
    "        Returns:\n",
    "        Structured JSON with score and explanations.\n",
    "        \"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are an expert academic reviewer participating in a post-publication\n",
    "citation arbitration process.\n",
    "\n",
    "Your task is to reassess the semantic relevance of a specific citation\n",
    "between two papers.\n",
    "\n",
    "The goal is NOT to evaluate overall paper similarity, nor to judge whether\n",
    "the cited paper is central to the entire citing paper.\n",
    "Instead, you must evaluate whether THIS PARTICULAR CITATION is\n",
    "semantically justified and substantively supported.\n",
    "\n",
    "The citation context below indicates WHERE and HOW the cited paper\n",
    "is referenced in the citing paper.\n",
    "Use this context as an anchor to locate the relevant discussion\n",
    "within the full text of the citing paper.\n",
    "\n",
    "You may consult the full text of both papers to determine whether\n",
    "the citation is meaningfully supported beyond the local context,\n",
    "including methodological usage, theoretical grounding,\n",
    "empirical comparison, or conceptual dependency.\n",
    "\n",
    "Ignore citation counts, venue prestige, and author identities.\n",
    "Focus strictly on semantic justification of this citation.\n",
    "\n",
    "---\n",
    "\n",
    "Citation context (attention anchor):\n",
    "\n",
    "\\r\\n\n",
    "{contexts}\n",
    "\\r\\n\n",
    "\n",
    "Citing paper (full text):\n",
    "\n",
    "\\r\\n\n",
    "{citing_text}\n",
    "\\r\\n\n",
    "\n",
    "Cited paper (full text):\n",
    "\n",
    "\\r\\n\n",
    "{cited_text}\n",
    "\\r\\n\n",
    "\n",
    "---\n",
    "\n",
    "Return your assessment strictly in the following JSON format:\n",
    "\n",
    "{{\n",
    "  \"score\": 0.0,\n",
    "  \"methodology\": \"Whether the cited paper provides methods, algorithms, or tools used in this citation.\",\n",
    "  \"theory\": \"Whether the cited paper contributes theoretical or conceptual foundations relevant to this citation.\",\n",
    "  \"evidence\": \"Whether the cited paper supplies empirical evidence, benchmarks, or comparisons relevant to this citation.\",\n",
    "  \"dependency\": \"Overall assessment of how substantively this citation depends on the cited paper.\",\n",
    "  \"summary\": \"One-sentence justification of the final relevance judgment.\"\n",
    "}}\n",
    "\n",
    "The score must be a real number between 0 and 1, where:\n",
    "- 0 indicates a weak, perfunctory, or nominal citation, and\n",
    "- 1 indicates a strongly justified and semantically substantial citation.\n",
    "\n",
    "Each field must contain a concise and concrete explanation.\n",
    "Do NOT include any text outside the JSON object.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a careful, neutral, and consistent academic reviewer.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            # temperature=0.0  # arbitration 必须可复现\n",
    "        )\n",
    "        try:\n",
    "            result = json.loads(response.choices[0].message.content)\n",
    "        except Exception as e:\n",
    "            result = {\n",
    "                'score': 0,\n",
    "                'methodology':'',\n",
    "                'theory':'',\n",
    "                'evidence':'',\n",
    "                'dependency':'',\n",
    "                'summary': str(e)\n",
    "            }\n",
    "\n",
    "        # safety clamp\n",
    "        result[\"score\"] = float(\n",
    "            min(max(result.get(\"score\", 0.0), 0.0), 1.0)\n",
    "        )\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e884dd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RescoreProsssor:\n",
    "\n",
    "  def __init__(self, obj, citing:str, cited:str):\n",
    "    self._scorer = LLMRescorer()\n",
    "    self._df = pd.DataFrame([{\n",
    "      'score': obj.llm,\n",
    "      'contexts': obj.contexts,\n",
    "      'abstract': obj.cited_abstract,\n",
    "      'methodology': obj.methodology,\n",
    "      'theory': obj.theory,\n",
    "      'evidence': obj.evidence,\n",
    "      'dependency': obj.dependency,\n",
    "      'summary': obj.summary\n",
    "    }])\n",
    "    self._contexts = obj.contexts\n",
    "    self._citing = self.pdf_to_text_grobid(citing)\n",
    "    self._cited = self.pdf_to_text_grobid(cited)\n",
    "  \n",
    "  def get_df(self):\n",
    "    return self._df\n",
    "\n",
    "  def pdf_to_text_grobid(self, pdf_path):\n",
    "    with open(pdf_path, \"rb\") as f:\n",
    "        r = requests.post(\n",
    "            \"http://localhost:8070/api/processFulltextDocument\",\n",
    "            files={\"input\": f}\n",
    "        )\n",
    "    return r.text  # TEI XML\n",
    "\n",
    "  def repeat(self, filename:str, n = 10):\n",
    "    with tqdm(total=n, desc=\"处理中\") as pbar:\n",
    "      while n > 0:\n",
    "        res = self._scorer.rescore(self._contexts, self._citing, self._cited)\n",
    "        self._df = pd.concat([self._df, pd.DataFrame([res])], ignore_index=True)\n",
    "        self._df.to_excel(filename)\n",
    "        pbar.set_postfix({\"n\": n, 'score':res.get('score', 0)})\n",
    "        pbar.update()  # 更新1个单位\n",
    "        n -= 1\n",
    "    pbar.close()\n",
    "    return self._df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50378b83",
   "metadata": {},
   "source": [
    "## resocre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c728d0",
   "metadata": {},
   "source": [
    "- high_cross_high_llm: 551\n",
    "- high_cross_mid_llm: 21\n",
    "- high_cross_low_llm: 701\n",
    "\n",
    "- low_cross_low_llm: 472\n",
    "- low_cross_mid_llm: 640\n",
    "- low_cross_high_llm: 109"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd183de",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 109\n",
    "data = data_instance.get_row_by_index(index)\n",
    "instance = RescoreProsssor(data, f'rescore/{index}citing.pdf', f'rescore/{index}cited.pdf')\n",
    "instance.repeat(f'rescore/{index}.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1591c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ScoreShower:\n",
    "  def __init__(self):\n",
    "    self._cols = {\n",
    "      'high_cross_high_llm': 551,\n",
    "      'high_cross_mid_llm': 21,\n",
    "      'high_cross_low_llm': 701,\n",
    "\n",
    "      'low_cross_low_llm': 472,\n",
    "      'low_cross_mid_llm': 640,\n",
    "      'low_cross_high_llm': 109\n",
    "    }\n",
    "    self._df = pd.DataFrame()\n",
    "  def plot(self):\n",
    "    for k, v in self._cols.items():\n",
    "      df = pd.read_excel(f'rescore/{v}.xlsx')\n",
    "      self._df[k] = df['score'].values\n",
    "    fig, ax = plt.subplots(figsize=(7.2, 5))\n",
    "    self._df.plot(ax=ax)\n",
    "    ax.set_xlabel(r\"$t$\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    return self._df\n",
    "ss = ScoreShower()\n",
    "ss.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df06a078",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
