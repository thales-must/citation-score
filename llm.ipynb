{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "600d83f3",
   "metadata": {},
   "source": [
    "# LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4d0720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from time import sleep, time\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e099db6b",
   "metadata": {},
   "source": [
    "## Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cecfe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMScorer:\n",
    "    \"\"\"\n",
    "    LLM-based semantic relevance scorer\n",
    "    L_ij = F_LLM(c_ij, a_j)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.client = OpenAI(api_key=os.getenv('LLM_KEY'), base_url=os.getenv('LLM_URL'))\n",
    "        self.model = os.getenv('LLM_MODEL')\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def score(self, context: str, cited_abstract: str) -> dict:\n",
    "        \"\"\"\n",
    "        Returns a structured JSON with:\n",
    "        score, methodology, theory, evidence, dependency, summary\n",
    "        \"\"\"\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "You are an expert academic reviewer.\n",
    "\n",
    "Your task is to evaluate the semantic relevance of a citation between two papers.\n",
    "\n",
    "Citing context:\n",
    "{context}\n",
    "\n",
    "Abstract of the cited paper:\n",
    "{cited_abstract}\n",
    "\n",
    "Please assess how strongly the cited paper supports, informs, or is essential\n",
    "to the citing paper in this context.\n",
    "\n",
    "Focus on semantic relevance rather than surface similarity.\n",
    "Do NOT consider citation counts, venue prestige, or author identity.\n",
    "\n",
    "Return your answer strictly in the following JSON format:\n",
    "\n",
    "{{\n",
    "  \"score\": 0.0,\n",
    "  \"methodology\": \"Brief justification on whether the cited paper is used as a method or tool.\",\n",
    "  \"theory\": \"Brief justification on theoretical or conceptual grounding.\",\n",
    "  \"evidence\": \"Brief justification on empirical support or comparison.\",\n",
    "  \"dependency\": \"Brief justification on how essential the cited work is to the citing paper.\",\n",
    "  \"summary\": \"One-sentence overall explanation of the relevance.\"\n",
    "}}\n",
    "\n",
    "The score must be a real number between 0 and 1.\n",
    "Each field must contain a concise but concrete justification.\n",
    "Do not include any text outside the JSON object.\n",
    "\"\"\"\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a careful and neutral academic reviewer.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            # temperature=0.0,   # 保证可复现\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        try:\n",
    "            result = json.loads(response.choices[0].message.content)\n",
    "        except Exception as e:\n",
    "            result = {\n",
    "                'score': 0,\n",
    "                'methodology':'',\n",
    "                'theory':'',\n",
    "                'evidence':'',\n",
    "                'dependency':'',\n",
    "                'summary': str(e)\n",
    "            }\n",
    "\n",
    "        # 安全兜底：保证 score 合法\n",
    "        result[\"score\"] = float(min(max(result.get(\"score\", 0.0), 0.0), 1.0))\n",
    "        return result\n",
    "\n",
    "    def abstract(self, citing_abstract:str, cited_abstract:str) -> dict:\n",
    "        \"\"\"\n",
    "        context not available\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "You are an expert academic reviewer.\n",
    "\n",
    "Your task is to assess the potential semantic relatedness between two research papers,\n",
    "in the absence of an explicit citation context.\n",
    "\n",
    "Abstract of the citing paper:\n",
    "{citing_abstract}\n",
    "\n",
    "Abstract of the cited paper:\n",
    "{cited_abstract}\n",
    "\n",
    "Please evaluate to what extent the cited paper is thematically, methodologically,\n",
    "or conceptually related to the citing paper at a global level.\n",
    "\n",
    "Note that you are NOT asked to infer citation intent.\n",
    "Instead, assess whether the two papers are meaningfully connected in terms of\n",
    "research topic, methods, or theoretical foundations.\n",
    "\n",
    "Return your answer strictly in the following JSON format:\n",
    "\n",
    "{{\n",
    "  \"score\": 0.0,\n",
    "  \"topic_overlap\": \"...\",\n",
    "  \"methodological_similarity\": \"...\",\n",
    "  \"conceptual_relation\": \"...\",\n",
    "  \"summary\": \"...\"\n",
    "}}\n",
    "\n",
    "The score must be a real number between 0 and 1.\n",
    "\n",
    "A higher score indicates stronger overall semantic relatedness,\n",
    "but does NOT imply that the citation is essential or central.\n",
    "\n",
    "Do not include any text outside the JSON object.\n",
    "        \"\"\"\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=self.model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a careful and neutral academic reviewer.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.0,   # 保证可复现\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        try:\n",
    "            result = json.loads(response.choices[0].message.content)\n",
    "        except Exception as e:\n",
    "            result = {\n",
    "                'score': 0,\n",
    "                'methodology':'',\n",
    "                'theory':'',\n",
    "                'evidence':'',\n",
    "                'dependency':'',\n",
    "                'summary': str(e)\n",
    "            }\n",
    "\n",
    "        # 安全兜底：保证 score 合法\n",
    "        result[\"score\"] = float(min(max(result.get(\"score\", 0.0), 0.0), 1.0))\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c51225",
   "metadata": {},
   "source": [
    "## init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8be541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = LLMScorer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9553da",
   "metadata": {},
   "source": [
    "## data definted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed23a3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoredf = pd.read_excel('score.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29086cf",
   "metadata": {},
   "source": [
    "## single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65346e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = scorer.get_model()\n",
    "iterator = 1\n",
    "data = scoredf.iloc[iterator]\n",
    "res = scorer.score(data.contexts, data.cited_abstract)\n",
    "scoredf.loc[iterator, [col, 'methodology', 'theory', 'evidence', 'dependency', 'summary']] = [res.get('score', 0), res.get('methodology', ''), res.get('theory', ''), res.get('evidence', ''), res.get('dependency', ''), res.get('summary', '')]\n",
    "scoredf.to_excel('score.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c16fbb",
   "metadata": {},
   "source": [
    "# context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4543483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = scorer.get_model()\n",
    "scoredf[col] = 0.0\n",
    "end = len(scoredf)\n",
    "iterator = 0\n",
    "\n",
    "with tqdm(total=end, desc=\"处理中\") as pbar:\n",
    "  while iterator < end:\n",
    "    row = scoredf.iloc[iterator]\n",
    "    res = scorer.score(row['contexts'], row['cited_abstract'])\n",
    "    scoredf.loc[iterator, [col]] = [res.get('score', 0)]\n",
    "    scoredf.loc[iterator, [col, 'methodology', 'theory', 'evidence', 'dependency', 'summary']] = [res.get('score', 0), res.get('methodology', ''), res.get('theory', ''), res.get('evidence', ''), res.get('dependency', ''), res.get('summary', '')]\n",
    "    # 可以更新额外信息\n",
    "    pbar.set_postfix({\"counter\": iterator, 'title':row['title'], 'score':res.get('score', 0)})\n",
    "    pbar.update(1)  # 更新1个单位\n",
    "    scoredf.to_excel('score.xlsx')\n",
    "    iterator += 1\n",
    "pbar.close()\n",
    "print(\"处理完成\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac5801d",
   "metadata": {},
   "source": [
    "## abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8878c2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = scorer.get_model() + '-abstract'\n",
    "scoredf[col] = 0.0\n",
    "end = len(scoredf)\n",
    "iterator = 0\n",
    "\n",
    "with tqdm(total=end, desc=\"处理中\") as pbar:\n",
    "  while iterator < end:\n",
    "    row = scoredf.iloc[iterator]\n",
    "    res = scorer.abstract(row['abstract'], row['cited_abstract'])\n",
    "    scoredf.loc[iterator, [col]] = [res.get('score', 0)]\n",
    "    # 可以更新额外信息\n",
    "    pbar.set_postfix({\"counter\": iterator, 'title':row['title'], 'score':res.get('score', 0)})\n",
    "    pbar.update(1)  # 更新1个单位\n",
    "    scoredf.to_excel('score.xlsx')\n",
    "    iterator += 1\n",
    "pbar.close()\n",
    "print(\"处理完成\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "latex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
